[global]
task      = CMSSW
backend   = condor
workdir   =/storage/a/harrendorf/SamplesForLHC/workdir ; Set workdir, use /storage since CMSSW tarball can become really large
cmdargs   = -G -c -m 10 ; -G gui -c run continously -m resubmit failing jobs 10 times, otherwise infinite 


[condor]
; JDLData = Requirements=(TARGET.CloudSite=="EKPSUPERMACHINES") docker_image=mschnepf/slc6-condocker
; universe = docker   ; run jobs on EKP desktop cloud
JDLData = Requirements=(TARGET.CloudSite=="BWFORCLUSTER") +RemoteJob=True ; run jobs on cluster in Freiburg
poolArgs req =
  walltimeMin => +RequestWalltime
proxy = VomsProxy


[jobs]
wall time = 2:00   ; how long jobs can run
in flight = 2000   ; how many jobs will be submitted at the same time
memory    = 2000


[CMSSW]
prepare config          = True
project area            = /storage/a/harrendorf/SamplesForLHC/CMSSW_8_0_19 ; choose your own local CMSSW installation as environment
se runtime              = True
config file        = /storage/a/harrendorf/SamplesForLHC/CMSSW_8_0_19/src/BoostedTTH/BoostedProducer/test/boostedProducer_cfg.py ; Give path to CMSSW config
dataset                 = /ttHTobb_M125_TuneCUETP8M2_ttHtranche3_13TeV-powheg-pythia8/RunIISpring16MiniAODv2-premix_withHLT_80X_mcRun2_asymptotic_v14-v1/MINIAODSIM
; if you want to give more than one dataset, you need a blank line after dataset = like
; dataset = 
;  /ttHTobb1
; /ttHTobb2  
dataset refresh       = 4:00 ; Define how often grid-control should check for new data sets
dataset splitter        = FileBoundarySplitter ; split dataset by files
files per job           = 2
depends                 = glite
partition lfn modifier = <xrootd:eu> ; important choose via which site / method input files are accessible, xrootd:eu should be convenient in most cases
partition lfn modifier dict =
    <srm:nrg> => srm://dgridsrm-fzk.gridka.de:8443/srm/managerv2?SFN=/pnfs/gridka.de/dcms/disk-only
    <xrootd>    => root://cms-xrd-global.cern.ch//
    <xrootd:eu> => root://xrootd-cms.infn.it//
    <xrootd:us> => root://cmsxrootd.fnal.gov//
    <xrootd:gridka> => root://cmsxrootd.gridka.de//
    <xrootd:desy> => root://dcache-cms-xrootd.desy.de:1094//
    <dcap:gridka> => dcap://dccmsdcap.gridka.de:22125/pnfs/gridka.de/cms/disk-only


[storage]
se output files   = *.root ; define which files should be stored, in general *.root should be sufficient
se path           = srm://dgridsrm-fzk.gridka.de:8443/srm/managerv2?SFN=/pnfs/gridka.de/dcms/disk-only/store/user/mharrend/
se output pattern = SamplesForHC/Production/#DATASETNAME#/#SAMPLENAME#_@GC_JOB_ID@.@XEXT@ ; change output pattern according to your needs, but keep @GC_JOB_ID@


[constants]
HOME =
GC_GLITE_LOCATION  = /cvmfs/grid.cern.ch/emi3ui-latest/etc/profile.d/setup-ui-example.sh ; grid environment which should be used
